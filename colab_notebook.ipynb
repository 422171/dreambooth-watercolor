{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CMpgHrboXLtm"
   },
   "source": [
    "#Stable Diffusion LoRA Fine-Tuning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QPldE4pIK5fU"
   },
   "source": [
    "##Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IwOWveJHMZk9"
   },
   "source": [
    "###Installing and importing all necessary libraries and modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 101044,
     "status": "ok",
     "timestamp": 1746263614303,
     "user": {
      "displayName": "Lahari Kethu",
      "userId": "01960283194028296161"
     },
     "user_tz": -330
    },
    "id": "wYBkO6v1WEES",
    "outputId": "4d4797fa-ab84-4f58-dcb0-7df78037df7d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m76.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m62.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m37.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m92.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m491.4/491.4 kB\u001b[0m \u001b[31m40.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -qq transformers peft accelerate torch torchvision safetensors datasets wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 7899,
     "status": "ok",
     "timestamp": 1746263634928,
     "user": {
      "displayName": "Lahari Kethu",
      "userId": "01960283194028296161"
     },
     "user_tz": -330
    },
    "id": "aLaYaOJAkggR"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from datasets import load_dataset\n",
    "from PIL import Image\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1XHTxvrKMHsO"
   },
   "source": [
    "##Dataset preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LlYg0WIIK_iO"
   },
   "source": [
    "###Preparing a small dataset of 4 images (loaded from an existing Hugging Face dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 209,
     "referenced_widgets": [
      "04bb3701902147a18484b87155f1ceca",
      "96130f0ebb3c42778c4ca3f7ff8aa9ee",
      "04f26c5561fa4c16be7d537150b20dcc",
      "da23b26220ca445d8db52fe4d8f03bce",
      "d66cc302d58a4becaca63ec3e918e1b7",
      "bd35e7e11a14467ba62a9df0ee812662",
      "5a6a3394b788463ea9dc6753dd46ad5c",
      "b8648ede84374cf8b6354667699a6e28",
      "75407779fb0640588bdba7d9dea04fbc",
      "f6d8922891874399bf3b876b377cba8d",
      "473b65d7aaa04e1dbf783bf988ba04e2",
      "fd89e828a5a2471c80fda1c6238ad565",
      "574db4ccedf54b5baff69e953ce38da8",
      "1e1591acdde846b4804d850d0f12dee0",
      "bb0d95af89004b27b910485e9a9d8a8c",
      "d48aa317c05a4eb1aa4e65d3f5318371",
      "79b8b860e477457cbdac7e99b8592b03",
      "661d6cf09cb7468d887f3d9399cdb315",
      "135be93423664f5caf2f688d97cd585f",
      "16ae79899ca44c76b61fd47deafd3e56",
      "6517015c2a67455ea931af1227a170a9",
      "d8c7d4fee9d3412d9b27e43ef9eb5774",
      "5d55a50df34a4864a8eb497ef77fcf8c",
      "35adde85a0a24fa6b3f4a215cbc6eaa9",
      "232b75fac6f346b5816ba35b48c8a10e",
      "6cbb9f088ed443bc976339c4a3fa6954",
      "7196c79c5f34451ca93e7e6905249652",
      "b1566f874cb6463696a69ff0247c0088",
      "1e66cc0d14d245d9a2562fa8b80b65c3",
      "db55f34360764669bf39299b793d0cef",
      "8ff2bdba191648e1885c476fcc586eff",
      "ac18d3d8bea94105aa74b5ace0243535",
      "3ec487e153ca439a8b6c58785743a419",
      "ae769c0d719a47ca8f22058815f96b78",
      "2fec13b53b66420e9218a4c46ca39693",
      "9d2a0e7b72c94f93a237f7faad6de3a3",
      "76bcbb384e584f88a03ef34f195ec099",
      "b5228dd60e1341cc88085253eb6f388a",
      "a7d221058cdb41d5bd7c32b359f85c08",
      "2ce5a78f81a2485e9a5898f5a99141c3",
      "cd4d03780cae4c3c90f9431d279d3c91",
      "54f8a3136c1d4217a826348bd8df91d3",
      "8af6af3038eb49e595df941db2fcbaa3",
      "8c7b256d8ed24db4848de511f642d402",
      "8ced7daf9a5f459581ca235fbda82a99",
      "93cc2c09cbce43439cccb7eb6f0ee33d",
      "76423c580c004017918a3169cb847168",
      "b0a655a6ac5c459aa31e1751d1340706",
      "e08c8a0d77364683960c29e285730d38",
      "a984aedc7378428b8108931dccc0dcf9",
      "efea44457f164a37ab23c381d90a865f",
      "7c412d50b03447f89ff91df33c1d4520",
      "138ac74cef2348c5a6f71dd0d8ee5fd9",
      "ae810e72850e4a5991cdf656b326ce5d",
      "ba06025ecff04df6976b4a8c760e1f60",
      "a573b06bc3e9489c8f84250822cea5b0",
      "7997c1ab2c6f4e09b542f154b22ea3a9",
      "f521db3f93754b69917f9e873c36239c",
      "9a147b25dada476485e84648e79c5b0c",
      "1bf76921ed2145d1aa04f3bbd27ae127",
      "f5caf9949511434faf22a63fe5c0babe",
      "4cd877a932904c3892ccaee9a5ff5c68",
      "0b186a9851624ed5b74cfbb86f0027b0",
      "00b5d091f4574333863589b8a73d77b3",
      "618926c9e4b84aff87ac15624962f71e",
      "381d2f5237064ff3ab53f3b4d8d3c2b4"
     ]
    },
    "executionInfo": {
     "elapsed": 7354,
     "status": "ok",
     "timestamp": 1746263644049,
     "user": {
      "displayName": "Lahari Kethu",
      "userId": "01960283194028296161"
     },
     "user_tz": -330
    },
    "id": "sS658iMwkjBi",
    "outputId": "2947fb09-29e8-4ab7-dd9c-00158f3ab370"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04bb3701902147a18484b87155f1ceca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "2021-004-GQ-British-001.jpg:   0%|          | 0.00/254k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd89e828a5a2471c80fda1c6238ad565",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "2021-005-M2-002-transformed.jpeg:   0%|          | 0.00/1.03M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d55a50df34a4864a8eb497ef77fcf8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "2021-005-M2-005-transformed.jpeg:   0%|          | 0.00/3.00M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae769c0d719a47ca8f22058815f96b78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "2021-005-M2-007.jpg:   0%|          | 0.00/219k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ced7daf9a5f459581ca235fbda82a99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tom-h-images-4.zip:   0%|          | 0.00/4.47M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a573b06bc3e9489c8f84250822cea5b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/8 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Step 1: Load dataset\n",
    "dataset = load_dataset(\"sandeshrajx/tom-hiddleston\", split=\"train[:4]\")\n",
    "\n",
    "# Step 2: Save images to directory\n",
    "os.makedirs(\"training_data/images\", exist_ok=True)\n",
    "\n",
    "for i, item in enumerate(dataset):\n",
    "    image: Image.Image = item[\"image\"]\n",
    "    filename = f\"image_{i}.jpg\"\n",
    "    image.save(f\"training_data/images/{filename}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m78DTCz0Lr2Y"
   },
   "source": [
    "##Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1nmVBRL-LDZ2"
   },
   "source": [
    "###Logging into Hugging Face and Weights & Biases using API tokens for authentication and experiment tracking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3318,
     "status": "ok",
     "timestamp": 1746263655524,
     "user": {
      "displayName": "Lahari Kethu",
      "userId": "01960283194028296161"
     },
     "user_tz": -330
    },
    "id": "z1IPyd8EsaUp",
    "outputId": "359fb043-447a-40dd-a997-c286c0e41f13"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdeidaratobi\u001b[0m (\u001b[33mdeidaratobi-national-institute-of-technology-andhra-pradesh\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "from google.colab import userdata\n",
    "\n",
    "HF_TOKEN = userdata.get('HF_TOKEN')\n",
    "login(token=HF_TOKEN)\n",
    "WANDB_API_KEY = userdata.get('WANDB_API_KEY')\n",
    "wandb.login(key=WANDB_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FgPC_OMRLHKK"
   },
   "source": [
    "###Cloning the Diffusers GitHub repository to access training utilities and scripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16782,
     "status": "ok",
     "timestamp": 1746263673358,
     "user": {
      "displayName": "Lahari Kethu",
      "userId": "01960283194028296161"
     },
     "user_tz": -330
    },
    "id": "KrETeeIUvlEv",
    "outputId": "ff73a611-b294-47d3-e2fa-574312eaffcc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/huggingface/diffusers\n",
      "  Cloning https://github.com/huggingface/diffusers to /tmp/pip-req-build-f6ct0bm5\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/diffusers /tmp/pip-req-build-f6ct0bm5\n",
      "  Resolved https://github.com/huggingface/diffusers to commit ec3d58286d72e8193b062f2bce7340ddd4c4defb\n",
      "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: importlib_metadata in /usr/local/lib/python3.11/dist-packages (from diffusers==0.34.0.dev0) (8.7.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from diffusers==0.34.0.dev0) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.27.0 in /usr/local/lib/python3.11/dist-packages (from diffusers==0.34.0.dev0) (0.30.2)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from diffusers==0.34.0.dev0) (2.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from diffusers==0.34.0.dev0) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from diffusers==0.34.0.dev0) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.11/dist-packages (from diffusers==0.34.0.dev0) (0.5.3)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from diffusers==0.34.0.dev0) (11.2.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.27.0->diffusers==0.34.0.dev0) (2025.3.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.27.0->diffusers==0.34.0.dev0) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.27.0->diffusers==0.34.0.dev0) (6.0.2)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.27.0->diffusers==0.34.0.dev0) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.27.0->diffusers==0.34.0.dev0) (4.13.2)\n",
      "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib_metadata->diffusers==0.34.0.dev0) (3.21.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->diffusers==0.34.0.dev0) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->diffusers==0.34.0.dev0) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->diffusers==0.34.0.dev0) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->diffusers==0.34.0.dev0) (2025.4.26)\n",
      "Building wheels for collected packages: diffusers\n",
      "  Building wheel for diffusers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for diffusers: filename=diffusers-0.34.0.dev0-py3-none-any.whl size=3602897 sha256=2dfef5516c19f2b992dd6725da22ca20b198ba22735460d949636ce474ddb083\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-cb8463eb/wheels/90/fb/48/a310c271ab42899362ff272062ced42133e5c4c9d0ce77df68\n",
      "Successfully built diffusers\n",
      "Installing collected packages: diffusers\n",
      "  Attempting uninstall: diffusers\n",
      "    Found existing installation: diffusers 0.33.1\n",
      "    Uninstalling diffusers-0.33.1:\n",
      "      Successfully uninstalled diffusers-0.33.1\n",
      "Successfully installed diffusers-0.34.0.dev0\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/huggingface/diffusers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8235,
     "status": "ok",
     "timestamp": 1746263761531,
     "user": {
      "displayName": "Lahari Kethu",
      "userId": "01960283194028296161"
     },
     "user_tz": -330
    },
    "id": "CKM7nqmi3gNr",
    "outputId": "26997dec-818e-4c42-a84e-0bb317d09f27"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'diffusers'...\n",
      "remote: Enumerating objects: 91589, done.\u001b[K\n",
      "remote: Counting objects: 100% (241/241), done.\u001b[K\n",
      "remote: Compressing objects: 100% (115/115), done.\u001b[K\n",
      "remote: Total 91589 (delta 175), reused 127 (delta 125), pack-reused 91348 (from 2)\u001b[K\n",
      "Receiving objects: 100% (91589/91589), 67.40 MiB | 25.34 MiB/s, done.\n",
      "Resolving deltas: 100% (67369/67369), done.\n",
      "/content/diffusers\n",
      "/content/diffusers/examples/dreambooth\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/huggingface/diffusers.git\n",
    "%cd diffusers\n",
    "%cd examples/dreambooth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b5z5uLRWLL-p"
   },
   "source": [
    "###Training the Stable Diffusion model using LoRA to apply lightweight, parameter-efficient fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 819118,
     "status": "ok",
     "timestamp": 1746264588022,
     "user": {
      "displayName": "Lahari Kethu",
      "userId": "01960283194028296161"
     },
     "user_tz": -330
    },
    "id": "6KKKpIC2mF6R",
    "outputId": "68d429a3-9b56-4ff6-ca4c-be9f11a231ae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following values were not passed to `accelerate launch` and had defaults used instead:\n",
      "\t`--num_processes` was set to a value of `1`\n",
      "\t`--num_machines` was set to a value of `1`\n",
      "\t`--mixed_precision` was set to a value of `'no'`\n",
      "\t`--dynamo_backend` was set to a value of `'no'`\n",
      "To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n",
      "2025-05-03 09:16:29.803255: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1746263790.045137    1955 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1746263790.103176    1955 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-05-03 09:16:30.562207: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "05/03/2025 09:16:38 - INFO - __main__ - Distributed environment: DistributedType.NO\n",
      "Num processes: 1\n",
      "Process index: 0\n",
      "Local process index: 0\n",
      "Device: cuda\n",
      "\n",
      "Mixed precision type: no\n",
      "\n",
      "tokenizer_config.json: 100% 806/806 [00:00<00:00, 5.47MB/s]\n",
      "vocab.json: 100% 1.06M/1.06M [00:00<00:00, 7.44MB/s]\n",
      "merges.txt: 100% 525k/525k [00:00<00:00, 7.19MB/s]\n",
      "special_tokens_map.json: 100% 472/472 [00:00<00:00, 2.98MB/s]\n",
      "config.json: 100% 617/617 [00:00<00:00, 3.62MB/s]\n",
      "You are using a model of type clip_text_model to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\n",
      "scheduler_config.json: 100% 308/308 [00:00<00:00, 1.55MB/s]\n",
      "{'dynamic_thresholding_ratio', 'clip_sample_range', 'sample_max_value', 'prediction_type', 'thresholding', 'rescale_betas_zero_snr', 'variance_type', 'timestep_spacing'} was not found in config. Values will be initialized to default values.\n",
      "model.safetensors: 100% 492M/492M [00:07<00:00, 69.7MB/s]\n",
      "config.json: 100% 547/547 [00:00<00:00, 3.28MB/s]\n",
      "diffusion_pytorch_model.safetensors: 100% 335M/335M [00:02<00:00, 161MB/s]\n",
      "{'shift_factor', 'latents_std', 'scaling_factor', 'use_post_quant_conv', 'force_upcast', 'mid_block_add_attention', 'use_quant_conv', 'latents_mean'} was not found in config. Values will be initialized to default values.\n",
      "All model checkpoint weights were used when initializing AutoencoderKL.\n",
      "\n",
      "All the weights of AutoencoderKL were initialized from the model checkpoint at runwayml/stable-diffusion-v1-5.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use AutoencoderKL for predictions without further training.\n",
      "config.json: 100% 743/743 [00:00<00:00, 5.52MB/s]\n",
      "diffusion_pytorch_model.safetensors: 100% 3.44G/3.44G [00:35<00:00, 96.9MB/s]\n",
      "{'use_linear_projection', 'upcast_attention', 'cross_attention_norm', 'resnet_time_scale_shift', 'encoder_hid_dim', 'resnet_out_scale_factor', 'mid_block_only_cross_attention', 'dual_cross_attention', 'addition_embed_type', 'encoder_hid_dim_type', 'projection_class_embeddings_input_dim', 'resnet_skip_time_act', 'time_embedding_type', 'addition_time_embed_dim', 'time_embedding_act_fn', 'only_cross_attention', 'num_attention_heads', 'time_embedding_dim', 'class_embeddings_concat', 'transformer_layers_per_block', 'conv_out_kernel', 'class_embed_type', 'num_class_embeds', 'dropout', 'time_cond_proj_dim', 'mid_block_type', 'conv_in_kernel', 'timestep_post_act', 'attention_type', 'reverse_transformer_layers_per_block', 'addition_embed_type_num_heads'} was not found in config. Values will be initialized to default values.\n",
      "All model checkpoint weights were used when initializing UNet2DConditionModel.\n",
      "\n",
      "All the weights of UNet2DConditionModel were initialized from the model checkpoint at runwayml/stable-diffusion-v1-5.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use UNet2DConditionModel for predictions without further training.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdeidaratobi\u001b[0m (\u001b[33mdeidaratobi-national-institute-of-technology-andhra-pradesh\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/diffusers/examples/dreambooth/wandb/run-20250503_091733-02jm4jb5\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mpeachy-silence-5\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/deidaratobi-national-institute-of-technology-andhra-pradesh/dreambooth-lora\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ğŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/deidaratobi-national-institute-of-technology-andhra-pradesh/dreambooth-lora/runs/02jm4jb5\u001b[0m\n",
      "05/03/2025 09:17:34 - INFO - __main__ - ***** Running training *****\n",
      "05/03/2025 09:17:34 - INFO - __main__ -   Num examples = 4\n",
      "05/03/2025 09:17:34 - INFO - __main__ -   Num batches each epoch = 4\n",
      "05/03/2025 09:17:34 - INFO - __main__ -   Num Epochs = 125\n",
      "05/03/2025 09:17:34 - INFO - __main__ -   Instantaneous batch size per device = 1\n",
      "05/03/2025 09:17:34 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "05/03/2025 09:17:34 - INFO - __main__ -   Gradient Accumulation steps = 1\n",
      "05/03/2025 09:17:34 - INFO - __main__ -   Total optimization steps = 500\n",
      "Steps:   1% 4/500 [00:05<09:55,  1.20s/it, loss=0.0115, lr=0.0001]\n",
      "model_index.json: 100% 541/541 [00:00<00:00, 2.05MB/s]\n",
      "\n",
      "Fetching 11 files:   0% 0/11 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "model.safetensors:   0% 0.00/1.22G [00:00<?, ?B/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "preprocessor_config.json: 100% 342/342 [00:00<00:00, 1.40MB/s]\n",
      "\n",
      "Fetching 11 files:   9% 1/11 [00:00<00:03,  2.52it/s]\u001b[A\n",
      "\n",
      "\n",
      "config.json: 100% 4.72k/4.72k [00:00<00:00, 10.4MB/s]\n",
      "\n",
      "\n",
      "model.safetensors:   2% 21.0M/1.22G [00:00<00:07, 166MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:   5% 62.9M/1.22G [00:00<00:04, 262MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:   9% 105M/1.22G [00:00<00:03, 290MB/s] \u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  11% 136M/1.22G [00:00<00:03, 274MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  14% 168M/1.22G [00:00<00:03, 272MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  16% 199M/1.22G [00:00<00:03, 277MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  19% 231M/1.22G [00:00<00:03, 284MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  22% 262M/1.22G [00:00<00:03, 285MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  24% 294M/1.22G [00:01<00:03, 283MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  27% 325M/1.22G [00:01<00:03, 286MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  29% 357M/1.22G [00:01<00:03, 281MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  32% 388M/1.22G [00:01<00:02, 284MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  34% 419M/1.22G [00:01<00:02, 272MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  37% 451M/1.22G [00:01<00:03, 246MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  40% 482M/1.22G [00:01<00:02, 251MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  42% 514M/1.22G [00:01<00:02, 255MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  45% 545M/1.22G [00:02<00:02, 250MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  47% 577M/1.22G [00:02<00:02, 251MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  50% 608M/1.22G [00:02<00:02, 243MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  53% 640M/1.22G [00:02<00:02, 247MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  55% 671M/1.22G [00:02<00:02, 261MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  58% 703M/1.22G [00:02<00:01, 257MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  60% 734M/1.22G [00:02<00:01, 250MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  63% 765M/1.22G [00:02<00:02, 214MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  66% 797M/1.22G [00:03<00:03, 139MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  67% 818M/1.22G [00:03<00:04, 97.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  69% 839M/1.22G [00:09<00:26, 14.0MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  72% 870M/1.22G [00:09<00:16, 20.7MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  74% 902M/1.22G [00:09<00:10, 29.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  77% 933M/1.22G [00:09<00:06, 41.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  79% 965M/1.22G [00:09<00:04, 55.7MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  82% 996M/1.22G [00:10<00:03, 72.8MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  85% 1.03G/1.22G [00:10<00:02, 77.8MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  87% 1.06G/1.22G [00:10<00:01, 97.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  90% 1.09G/1.22G [00:10<00:01, 119MB/s] \u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  92% 1.12G/1.22G [00:10<00:00, 142MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  95% 1.15G/1.22G [00:10<00:00, 153MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  97% 1.18G/1.22G [00:11<00:00, 171MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors: 100% 1.22G/1.22G [00:11<00:00, 107MB/s]\n",
      "\n",
      "Fetching 11 files: 100% 11/11 [00:11<00:00,  1.06s/it]\n",
      "{'image_encoder', 'requires_safety_checker'} was not found in config. Values will be initialized to default values.\n",
      "\n",
      "Loading pipeline components...:   0% 0/7 [00:00<?, ?it/s]\u001b[AInstantiating AutoencoderKL model under default dtype torch.float32.\n",
      "{'shift_factor', 'latents_std', 'scaling_factor', 'use_post_quant_conv', 'force_upcast', 'mid_block_add_attention', 'use_quant_conv', 'latents_mean'} was not found in config. Values will be initialized to default values.\n",
      "All model checkpoint weights were used when initializing AutoencoderKL.\n",
      "\n",
      "All the weights of AutoencoderKL were initialized from the model checkpoint at /root/.cache/huggingface/hub/models--runwayml--stable-diffusion-v1-5/snapshots/451f4fe16113bff5a5d2269ed5ad43b0592e9a14/vae.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use AutoencoderKL for predictions without further training.\n",
      "Loaded vae as AutoencoderKL from `vae` subfolder of runwayml/stable-diffusion-v1-5.\n",
      "\n",
      "Loading pipeline components...:  14% 1/7 [00:03<00:18,  3.13s/it]\u001b[ALoaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of runwayml/stable-diffusion-v1-5.\n",
      "Loaded safety_checker as StableDiffusionSafetyChecker from `safety_checker` subfolder of runwayml/stable-diffusion-v1-5.\n",
      "\n",
      "Loading pipeline components...:  43% 3/7 [00:03<00:03,  1.06it/s]\u001b[A{'prediction_type', 'timestep_spacing'} was not found in config. Values will be initialized to default values.\n",
      "Loaded scheduler as PNDMScheduler from `scheduler` subfolder of runwayml/stable-diffusion-v1-5.\n",
      "Loaded feature_extractor as CLIPImageProcessor from `feature_extractor` subfolder of runwayml/stable-diffusion-v1-5.\n",
      "Loading pipeline components...: 100% 7/7 [00:03<00:00,  2.01it/s]\n",
      "05/03/2025 09:17:56 - INFO - __main__ - Running validation... \n",
      " Generating 4 images with prompt: A photo of sks <person> cooking pasta..\n",
      "{'use_exponential_sigmas', 'prediction_type', 'use_flow_sigmas', 'lambda_min_clipped', 'timestep_spacing', 'algorithm_type', 'dynamic_thresholding_ratio', 'lower_order_final', 'thresholding', 'final_sigmas_type', 'use_beta_sigmas', 'variance_type', 'sample_max_value', 'solver_order', 'solver_type', 'use_karras_sigmas', 'flow_shift', 'use_lu_lambdas', 'rescale_betas_zero_snr', 'euler_at_final'} was not found in config. Values will be initialized to default values.\n",
      "Steps:  20% 100/500 [02:46<07:03,  1.06s/it, loss=0.203, lr=0.0001]05/03/2025 09:20:21 - INFO - accelerate.accelerator - Saving current state to /content/lora_tom_model/checkpoint-100\n",
      "Model weights saved in /content/lora_tom_model/checkpoint-100/pytorch_lora_weights.safetensors\n",
      "05/03/2025 09:20:21 - INFO - accelerate.checkpointing - Optimizer state saved in /content/lora_tom_model/checkpoint-100/optimizer.bin\n",
      "05/03/2025 09:20:21 - INFO - accelerate.checkpointing - Scheduler state saved in /content/lora_tom_model/checkpoint-100/scheduler.bin\n",
      "05/03/2025 09:20:21 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in /content/lora_tom_model/checkpoint-100/sampler.bin\n",
      "05/03/2025 09:20:21 - INFO - accelerate.checkpointing - Random states saved in /content/lora_tom_model/checkpoint-100/random_states_0.pkl\n",
      "05/03/2025 09:20:21 - INFO - __main__ - Saved state to /content/lora_tom_model/checkpoint-100\n",
      "Steps:  40% 200/500 [04:34<05:13,  1.05s/it, loss=0.0589, lr=0.0001]05/03/2025 09:22:09 - INFO - accelerate.accelerator - Saving current state to /content/lora_tom_model/checkpoint-200\n",
      "Model weights saved in /content/lora_tom_model/checkpoint-200/pytorch_lora_weights.safetensors\n",
      "05/03/2025 09:22:09 - INFO - accelerate.checkpointing - Optimizer state saved in /content/lora_tom_model/checkpoint-200/optimizer.bin\n",
      "05/03/2025 09:22:09 - INFO - accelerate.checkpointing - Scheduler state saved in /content/lora_tom_model/checkpoint-200/scheduler.bin\n",
      "05/03/2025 09:22:09 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in /content/lora_tom_model/checkpoint-200/sampler.bin\n",
      "05/03/2025 09:22:09 - INFO - accelerate.checkpointing - Random states saved in /content/lora_tom_model/checkpoint-200/random_states_0.pkl\n",
      "05/03/2025 09:22:09 - INFO - __main__ - Saved state to /content/lora_tom_model/checkpoint-200\n",
      "Steps:  41% 204/500 [04:39<05:17,  1.07s/it, loss=0.118, lr=0.0001]  {'image_encoder', 'requires_safety_checker'} was not found in config. Values will be initialized to default values.\n",
      "\n",
      "Loading pipeline components...:   0% 0/7 [00:00<?, ?it/s]\u001b[AInstantiating AutoencoderKL model under default dtype torch.float32.\n",
      "{'shift_factor', 'latents_std', 'scaling_factor', 'use_post_quant_conv', 'force_upcast', 'mid_block_add_attention', 'use_quant_conv', 'latents_mean'} was not found in config. Values will be initialized to default values.\n",
      "All model checkpoint weights were used when initializing AutoencoderKL.\n",
      "\n",
      "All the weights of AutoencoderKL were initialized from the model checkpoint at /root/.cache/huggingface/hub/models--runwayml--stable-diffusion-v1-5/snapshots/451f4fe16113bff5a5d2269ed5ad43b0592e9a14/vae.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use AutoencoderKL for predictions without further training.\n",
      "Loaded vae as AutoencoderKL from `vae` subfolder of runwayml/stable-diffusion-v1-5.\n",
      "Loaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of runwayml/stable-diffusion-v1-5.\n",
      "\n",
      "Loading pipeline components...:  29% 2/7 [00:00<00:00, 18.69it/s]\u001b[ALoaded safety_checker as StableDiffusionSafetyChecker from `safety_checker` subfolder of runwayml/stable-diffusion-v1-5.\n",
      "{'prediction_type', 'timestep_spacing'} was not found in config. Values will be initialized to default values.\n",
      "Loaded scheduler as PNDMScheduler from `scheduler` subfolder of runwayml/stable-diffusion-v1-5.\n",
      "\n",
      "Loading pipeline components...:  57% 4/7 [00:00<00:00,  9.99it/s]\u001b[ALoaded feature_extractor as CLIPImageProcessor from `feature_extractor` subfolder of runwayml/stable-diffusion-v1-5.\n",
      "Loading pipeline components...: 100% 7/7 [00:00<00:00, 18.74it/s]\n",
      "05/03/2025 09:22:14 - INFO - __main__ - Running validation... \n",
      " Generating 4 images with prompt: A photo of sks <person> cooking pasta..\n",
      "{'use_exponential_sigmas', 'prediction_type', 'use_flow_sigmas', 'lambda_min_clipped', 'timestep_spacing', 'algorithm_type', 'dynamic_thresholding_ratio', 'lower_order_final', 'thresholding', 'final_sigmas_type', 'use_beta_sigmas', 'variance_type', 'sample_max_value', 'solver_order', 'solver_type', 'use_karras_sigmas', 'flow_shift', 'use_lu_lambdas', 'rescale_betas_zero_snr', 'euler_at_final'} was not found in config. Values will be initialized to default values.\n",
      "Steps:  60% 300/500 [07:04<03:30,  1.05s/it, loss=0.0165, lr=0.0001]05/03/2025 09:24:38 - INFO - accelerate.accelerator - Saving current state to /content/lora_tom_model/checkpoint-300\n",
      "Model weights saved in /content/lora_tom_model/checkpoint-300/pytorch_lora_weights.safetensors\n",
      "05/03/2025 09:24:39 - INFO - accelerate.checkpointing - Optimizer state saved in /content/lora_tom_model/checkpoint-300/optimizer.bin\n",
      "05/03/2025 09:24:39 - INFO - accelerate.checkpointing - Scheduler state saved in /content/lora_tom_model/checkpoint-300/scheduler.bin\n",
      "05/03/2025 09:24:39 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in /content/lora_tom_model/checkpoint-300/sampler.bin\n",
      "05/03/2025 09:24:39 - INFO - accelerate.checkpointing - Random states saved in /content/lora_tom_model/checkpoint-300/random_states_0.pkl\n",
      "05/03/2025 09:24:39 - INFO - __main__ - Saved state to /content/lora_tom_model/checkpoint-300\n",
      "Steps:  80% 400/500 [08:51<01:45,  1.05s/it, loss=0.0169, lr=0.0001]05/03/2025 09:26:26 - INFO - accelerate.accelerator - Saving current state to /content/lora_tom_model/checkpoint-400\n",
      "Model weights saved in /content/lora_tom_model/checkpoint-400/pytorch_lora_weights.safetensors\n",
      "05/03/2025 09:26:26 - INFO - accelerate.checkpointing - Optimizer state saved in /content/lora_tom_model/checkpoint-400/optimizer.bin\n",
      "05/03/2025 09:26:26 - INFO - accelerate.checkpointing - Scheduler state saved in /content/lora_tom_model/checkpoint-400/scheduler.bin\n",
      "05/03/2025 09:26:26 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in /content/lora_tom_model/checkpoint-400/sampler.bin\n",
      "05/03/2025 09:26:26 - INFO - accelerate.checkpointing - Random states saved in /content/lora_tom_model/checkpoint-400/random_states_0.pkl\n",
      "05/03/2025 09:26:26 - INFO - __main__ - Saved state to /content/lora_tom_model/checkpoint-400\n",
      "Steps:  81% 404/500 [08:56<01:43,  1.07s/it, loss=0.00538, lr=0.0001]{'image_encoder', 'requires_safety_checker'} was not found in config. Values will be initialized to default values.\n",
      "\n",
      "Loading pipeline components...:   0% 0/7 [00:00<?, ?it/s]\u001b[AInstantiating AutoencoderKL model under default dtype torch.float32.\n",
      "{'shift_factor', 'latents_std', 'scaling_factor', 'use_post_quant_conv', 'force_upcast', 'mid_block_add_attention', 'use_quant_conv', 'latents_mean'} was not found in config. Values will be initialized to default values.\n",
      "All model checkpoint weights were used when initializing AutoencoderKL.\n",
      "\n",
      "All the weights of AutoencoderKL were initialized from the model checkpoint at /root/.cache/huggingface/hub/models--runwayml--stable-diffusion-v1-5/snapshots/451f4fe16113bff5a5d2269ed5ad43b0592e9a14/vae.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use AutoencoderKL for predictions without further training.\n",
      "Loaded vae as AutoencoderKL from `vae` subfolder of runwayml/stable-diffusion-v1-5.\n",
      "Loaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of runwayml/stable-diffusion-v1-5.\n",
      "\n",
      "Loading pipeline components...:  29% 2/7 [00:00<00:00, 19.95it/s]\u001b[ALoaded safety_checker as StableDiffusionSafetyChecker from `safety_checker` subfolder of runwayml/stable-diffusion-v1-5.\n",
      "{'prediction_type', 'timestep_spacing'} was not found in config. Values will be initialized to default values.\n",
      "Loaded scheduler as PNDMScheduler from `scheduler` subfolder of runwayml/stable-diffusion-v1-5.\n",
      "\n",
      "Loading pipeline components...:  57% 4/7 [00:00<00:00, 10.87it/s]\u001b[ALoaded feature_extractor as CLIPImageProcessor from `feature_extractor` subfolder of runwayml/stable-diffusion-v1-5.\n",
      "Loading pipeline components...: 100% 7/7 [00:00<00:00, 19.40it/s]\n",
      "05/03/2025 09:26:32 - INFO - __main__ - Running validation... \n",
      " Generating 4 images with prompt: A photo of sks <person> cooking pasta..\n",
      "{'use_exponential_sigmas', 'prediction_type', 'use_flow_sigmas', 'lambda_min_clipped', 'timestep_spacing', 'algorithm_type', 'dynamic_thresholding_ratio', 'lower_order_final', 'thresholding', 'final_sigmas_type', 'use_beta_sigmas', 'variance_type', 'sample_max_value', 'solver_order', 'solver_type', 'use_karras_sigmas', 'flow_shift', 'use_lu_lambdas', 'rescale_betas_zero_snr', 'euler_at_final'} was not found in config. Values will be initialized to default values.\n",
      "Steps: 100% 500/500 [11:21<00:00,  1.06s/it, loss=0.00336, lr=0.0001]05/03/2025 09:28:56 - INFO - accelerate.accelerator - Saving current state to /content/lora_tom_model/checkpoint-500\n",
      "Model weights saved in /content/lora_tom_model/checkpoint-500/pytorch_lora_weights.safetensors\n",
      "05/03/2025 09:28:56 - INFO - accelerate.checkpointing - Optimizer state saved in /content/lora_tom_model/checkpoint-500/optimizer.bin\n",
      "05/03/2025 09:28:56 - INFO - accelerate.checkpointing - Scheduler state saved in /content/lora_tom_model/checkpoint-500/scheduler.bin\n",
      "05/03/2025 09:28:56 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in /content/lora_tom_model/checkpoint-500/sampler.bin\n",
      "05/03/2025 09:28:56 - INFO - accelerate.checkpointing - Random states saved in /content/lora_tom_model/checkpoint-500/random_states_0.pkl\n",
      "05/03/2025 09:28:56 - INFO - __main__ - Saved state to /content/lora_tom_model/checkpoint-500\n",
      "Steps: 100% 500/500 [11:21<00:00,  1.06s/it, loss=0.0463, lr=0.0001] Model weights saved in /content/lora_tom_model/pytorch_lora_weights.safetensors\n",
      "{'image_encoder', 'requires_safety_checker'} was not found in config. Values will be initialized to default values.\n",
      "\n",
      "Loading pipeline components...:   0% 0/7 [00:00<?, ?it/s]\u001b[AInstantiating AutoencoderKL model under default dtype torch.float32.\n",
      "{'shift_factor', 'latents_std', 'scaling_factor', 'use_post_quant_conv', 'force_upcast', 'mid_block_add_attention', 'use_quant_conv', 'latents_mean'} was not found in config. Values will be initialized to default values.\n",
      "All model checkpoint weights were used when initializing AutoencoderKL.\n",
      "\n",
      "All the weights of AutoencoderKL were initialized from the model checkpoint at /root/.cache/huggingface/hub/models--runwayml--stable-diffusion-v1-5/snapshots/451f4fe16113bff5a5d2269ed5ad43b0592e9a14/vae.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use AutoencoderKL for predictions without further training.\n",
      "Loaded vae as AutoencoderKL from `vae` subfolder of runwayml/stable-diffusion-v1-5.\n",
      "Loaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of runwayml/stable-diffusion-v1-5.\n",
      "\n",
      "Loading pipeline components...:  29% 2/7 [00:00<00:00, 16.99it/s]\u001b[ALoaded safety_checker as StableDiffusionSafetyChecker from `safety_checker` subfolder of runwayml/stable-diffusion-v1-5.\n",
      "{'prediction_type', 'timestep_spacing'} was not found in config. Values will be initialized to default values.\n",
      "Loaded scheduler as PNDMScheduler from `scheduler` subfolder of runwayml/stable-diffusion-v1-5.\n",
      "\n",
      "Loading pipeline components...:  57% 4/7 [00:00<00:00, 10.19it/s]\u001b[ALoaded text_encoder as CLIPTextModel from `text_encoder` subfolder of runwayml/stable-diffusion-v1-5.\n",
      "Instantiating UNet2DConditionModel model under default dtype torch.float32.\n",
      "{'use_linear_projection', 'upcast_attention', 'cross_attention_norm', 'resnet_time_scale_shift', 'encoder_hid_dim', 'resnet_out_scale_factor', 'mid_block_only_cross_attention', 'dual_cross_attention', 'addition_embed_type', 'encoder_hid_dim_type', 'projection_class_embeddings_input_dim', 'resnet_skip_time_act', 'time_embedding_type', 'addition_time_embed_dim', 'time_embedding_act_fn', 'only_cross_attention', 'num_attention_heads', 'time_embedding_dim', 'class_embeddings_concat', 'transformer_layers_per_block', 'conv_out_kernel', 'class_embed_type', 'num_class_embeds', 'dropout', 'time_cond_proj_dim', 'mid_block_type', 'conv_in_kernel', 'timestep_post_act', 'attention_type', 'reverse_transformer_layers_per_block', 'addition_embed_type_num_heads'} was not found in config. Values will be initialized to default values.\n",
      "All model checkpoint weights were used when initializing UNet2DConditionModel.\n",
      "\n",
      "All the weights of UNet2DConditionModel were initialized from the model checkpoint at /root/.cache/huggingface/hub/models--runwayml--stable-diffusion-v1-5/snapshots/451f4fe16113bff5a5d2269ed5ad43b0592e9a14/unet.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use UNet2DConditionModel for predictions without further training.\n",
      "Loaded unet as UNet2DConditionModel from `unet` subfolder of runwayml/stable-diffusion-v1-5.\n",
      "\n",
      "Loading pipeline components...:  86% 6/7 [00:01<00:00,  3.41it/s]\u001b[ALoaded feature_extractor as CLIPImageProcessor from `feature_extractor` subfolder of runwayml/stable-diffusion-v1-5.\n",
      "Loading pipeline components...: 100% 7/7 [00:01<00:00,  4.92it/s]\n",
      "Loading unet.\n",
      "No LoRA keys associated to CLIPTextModel found with the prefix='text_encoder'. This is safe to ignore if LoRA state dict didn't originally have any CLIPTextModel related params. You can also try specifying `prefix=None` to resolve the warning. Otherwise, open an issue if you think it's unexpected: https://github.com/huggingface/diffusers/issues/new\n",
      "05/03/2025 09:28:58 - INFO - __main__ - Running validation... \n",
      " Generating 4 images with prompt: A photo of sks <person> cooking pasta..\n",
      "{'use_exponential_sigmas', 'prediction_type', 'use_flow_sigmas', 'lambda_min_clipped', 'timestep_spacing', 'algorithm_type', 'dynamic_thresholding_ratio', 'lower_order_final', 'thresholding', 'final_sigmas_type', 'use_beta_sigmas', 'variance_type', 'sample_max_value', 'solver_order', 'solver_type', 'use_karras_sigmas', 'flow_shift', 'use_lu_lambdas', 'rescale_betas_zero_snr', 'euler_at_final'} was not found in config. Values will be initialized to default values.\n",
      "Potential NSFW content was detected in one or more images. A black image will be returned instead. Try again with a different prompt and/or seed.\n",
      "\n",
      "README.md: 100% 1.32k/1.32k [00:00<00:00, 3.36MB/s]\n",
      "\n",
      "optimizer.bin:   0% 0.00/6.59M [00:00<?, ?B/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "pytorch_lora_weights.safetensors:   0% 0.00/3.23M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:   0% 0.00/6.59M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "optimizer.bin:   0% 0.00/6.59M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Upload 14 LFS files:   0% 0/14 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_lora_weights.safetensors:   0% 0.00/3.23M [00:00<?, ?B/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "optimizer.bin:  51% 3.39M/6.59M [00:00<00:00, 33.9MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:   4% 279k/6.59M [00:00<00:02, 2.74MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_lora_weights.safetensors:  11% 344k/3.23M [00:00<00:01, 2.87MB/s]\u001b[A\u001b[A\n",
      "optimizer.bin:  76% 4.98M/6.59M [00:00<00:00, 24.1MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  39% 2.56M/6.59M [00:00<00:00, 12.7MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_lora_weights.safetensors:  51% 1.64M/3.23M [00:00<00:00, 7.29MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  83% 5.46M/6.59M [00:00<00:00, 17.3MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "optimizer.bin: 100% 6.59M/6.59M [00:00<00:00, 10.3MB/s]\n",
      "optimizer.bin: 100% 6.59M/6.59M [00:00<00:00, 9.23MB/s]\n",
      "pytorch_lora_weights.safetensors: 100% 3.23M/3.23M [00:00<00:00, 4.49MB/s]\n",
      "optimizer.bin: 100% 6.59M/6.59M [00:00<00:00, 8.47MB/s]\n",
      "\n",
      "pytorch_lora_weights.safetensors:   0% 0.00/3.23M [00:00<?, ?B/s]\u001b[A\n",
      "\n",
      "\n",
      "pytorch_lora_weights.safetensors: 100% 3.23M/3.23M [00:00<00:00, 3.57MB/s]\n",
      "\n",
      "\n",
      "pytorch_lora_weights.safetensors:   0% 0.00/3.23M [00:00<?, ?B/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Upload 14 LFS files:   7% 1/14 [00:00<00:12,  1.07it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:   0% 0.00/6.59M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Upload 14 LFS files:  14% 2/14 [00:01<00:05,  2.08it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "pytorch_lora_weights.safetensors:   0% 0.00/3.23M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  86% 5.69M/6.59M [00:00<00:00, 32.3MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "pytorch_lora_weights.safetensors: 100% 3.23M/3.23M [00:00<00:00, 7.66MB/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Upload 14 LFS files:  43% 6/14 [00:01<00:01,  5.81it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "pytorch_lora_weights.safetensors: 100% 3.23M/3.23M [00:00<00:00, 5.29MB/s]\n",
      "optimizer.bin: 100% 6.59M/6.59M [00:00<00:00, 9.82MB/s]\n",
      "optimizer.bin: 100% 6.59M/6.59M [00:00<00:00, 8.79MB/s]\n",
      "\n",
      "\n",
      "image_1.png:   0% 0.00/380k [00:00<?, ?B/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "image_3.png:   0% 0.00/360k [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Upload 14 LFS files:  50% 7/14 [00:01<00:01,  4.79it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "image_0.png: 100% 309k/309k [00:00<00:00, 811kB/s]\n",
      "pytorch_lora_weights.safetensors: 100% 3.23M/3.23M [00:00<00:00, 4.56MB/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "image_1.png: 100% 380k/380k [00:00<00:00, 965kB/s]\n",
      "image_3.png: 100% 360k/360k [00:00<00:00, 792kB/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Upload 14 LFS files:  86% 12/14 [00:02<00:00,  7.04it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "pytorch_lora_weights.safetensors: 100% 3.23M/3.23M [00:00<00:00, 5.29MB/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Upload 14 LFS files: 100% 14/14 [00:02<00:00,  5.62it/s]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: loss â–â–â–‚â–â–†â–â–â–â–‚â–‚â–„â–ƒâ–â–‚â–ƒâ–â–‚â–ˆâ–â–â–„â–‚â–â–‚â–ƒâ–â–‚â–â–â–ƒâ–â–‚â–â–ƒâ–†â–â–‚â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: loss 0.04625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   lr 0.0001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ğŸš€ View run \u001b[33mpeachy-silence-5\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/deidaratobi-national-institute-of-technology-andhra-pradesh/dreambooth-lora/runs/02jm4jb5\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/deidaratobi-national-institute-of-technology-andhra-pradesh/dreambooth-lora\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 16 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250503_091733-02jm4jb5/logs\u001b[0m\n",
      "Steps: 100% 500/500 [12:08<00:00,  1.46s/it, loss=0.0463, lr=0.0001]\n"
     ]
    }
   ],
   "source": [
    "!accelerate launch train_dreambooth_lora.py \\\n",
    "  --pretrained_model_name_or_path=\"runwayml/stable-diffusion-v1-5\" \\\n",
    "  --instance_data_dir=\"/content/training_data/images\" \\\n",
    "  --output_dir=\"/content/lora_tom_model\" \\\n",
    "  --instance_prompt=\"a photo of sks person\" \\\n",
    "  --resolution=512 \\\n",
    "  --train_batch_size=1 \\\n",
    "  --gradient_accumulation_steps=1 \\\n",
    "  --checkpointing_steps=100 \\\n",
    "  --learning_rate=1e-4 \\\n",
    "  --report_to=\"wandb\" \\\n",
    "  --lr_scheduler=\"constant\" \\\n",
    "  --lr_warmup_steps=0 \\\n",
    "  --max_train_steps=500 \\\n",
    "  --validation_prompt=\"A photo of sks person cooking pasta.\" \\\n",
    "  --validation_epochs=50 \\\n",
    "  --seed=42 \\\n",
    "  --push_to_hub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "brMzZL6MLvop"
   },
   "source": [
    "##Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3pDxGoKaLQyO"
   },
   "source": [
    "###Loading the fine-tuned checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87,
     "referenced_widgets": [
      "ad85edcf65d24aa483de508fb3034912",
      "617367ddef5f4527b021081ef0eeebdc",
      "af2860746a7c4b95bec8ee183b7fc670",
      "eb485b9af8674a90983fdf528ffee9c8",
      "7ebe8fea3df840d2a816fab5d5a4c0d1",
      "2090fb62b6d445b0aa3a4d94162f207a",
      "83a57e4f44994c93a83e0e488c7e00ac",
      "0097ceb02c904a9dba5c40c8c4c8f05e",
      "7c9df45bd66f4f64b9f418af76f98dd6",
      "ac7005bf36384e8aadad93447f108910",
      "c1e6c20b0be94efda4c52eb396ff3be6"
     ]
    },
    "executionInfo": {
     "elapsed": 29501,
     "status": "ok",
     "timestamp": 1746264955229,
     "user": {
      "displayName": "Lahari Kethu",
      "userId": "01960283194028296161"
     },
     "user_tz": -330
    },
    "id": "8XD_EY-SzVYN",
    "outputId": "027dfe12-61cf-4320-bb8a-009ded44d17e"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad85edcf65d24aa483de508fb3034912",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No LoRA keys associated to CLIPTextModel found with the prefix='text_encoder'. This is safe to ignore if LoRA state dict didn't originally have any CLIPTextModel related params. You can also try specifying `prefix=None` to resolve the warning. Otherwise, open an issue if you think it's unexpected: https://github.com/huggingface/diffusers/issues/new\n"
     ]
    }
   ],
   "source": [
    "from diffusers import DiffusionPipeline\n",
    "pipe = DiffusionPipeline.from_pretrained(\"runwayml/stable-diffusion-v1-5\").to(\"cuda\")\n",
    "pipe.load_lora_weights(\"/content/lora_tom_model/checkpoint-500\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 119,
     "status": "ok",
     "timestamp": 1746264994783,
     "user": {
      "displayName": "Lahari Kethu",
      "userId": "01960283194028296161"
     },
     "user_tz": -330
    },
    "id": "nUGC2faN8aec"
   },
   "outputs": [],
   "source": [
    "%mkdir /content/output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1746265031228,
     "user": {
      "displayName": "Lahari Kethu",
      "userId": "01960283194028296161"
     },
     "user_tz": -330
    },
    "id": "J6Y9jM76gaXq",
    "outputId": "7c1416a4-9079-48ed-c1af-312481f2c596"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/output\n"
     ]
    }
   ],
   "source": [
    "%cd /content/output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yYFXIlmkL7FH"
   },
   "source": [
    "###Running inference to generate new images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 113,
     "referenced_widgets": [
      "d9e7e66d7e3646fbae502fed52203332",
      "2742d340d5d8452cbd5459c1b5662bec",
      "5c1d760875514ed791349d531a013f5c",
      "b0206fb9cfec405a9b1501e36a86e923",
      "2462ea022a944f25b5ef91d40fb62f36",
      "cdb2dd53f59044c8a8f3fa68c4881ddf",
      "dd03a705a11a4413b50da4336f8a2244",
      "d3806a7c37844f3f805e095c78f33eff",
      "fdebdf9c95734566b770ad2225608dad",
      "804f1f1fd3094c69a7c6ec56fa8db71c",
      "baa290c77e9e49db9ae51c683c5826cf",
      "281229ca453240d48c2b24d2b30dbd22",
      "8c1d3926fe28422bb0b3c0eb1a7f51dd",
      "c89392d494f743de8cd7655d2389615a",
      "279db225187d4c16a3c7c57c7eca6c4f",
      "14e5cbcc9e3042f4a09ecc33006088f5",
      "77f9f90ebb614920b0ada2958a7a3265",
      "b186e18b59f34f65929d989103573b41",
      "efa82254d51b436491ea4b2169870d9d",
      "dbc4edc04d034c6787e35290fdcebc61",
      "92d17b81a0844957a07072da3fae51b5",
      "ef94dffe9c044db483637d4fcd61d1ef",
      "18a2c968fe6e4f1ebe73bda7a8254a98",
      "5a32516e96594653ba754380f84154b8",
      "8dc1e52dad094ad48e85fcf62b7cce64",
      "a80fd74725a64b8bba2f8e6e1dbc200b",
      "a07b4392eceb4e49ba01e83ee34a6d5b",
      "b587af03846f45dfb3b69582fb4793f0",
      "62f0ce513b254deaa2ecca3d4c191ac4",
      "22e27fbe61e64547850a9c2749b7f349",
      "aea563b5a61942f58077ebbfda2360bc",
      "fc446eaa77fd4058964f1546c9763dc1",
      "4fef7205e5ff41a687dc933024fbfc85"
     ]
    },
    "executionInfo": {
     "elapsed": 48606,
     "status": "ok",
     "timestamp": 1746265314386,
     "user": {
      "displayName": "Lahari Kethu",
      "userId": "01960283194028296161"
     },
     "user_tz": -330
    },
    "id": "_9GE505loXww",
    "outputId": "69db3c9e-309f-4caa-9a6f-edfddce865b2"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9e7e66d7e3646fbae502fed52203332",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "281229ca453240d48c2b24d2b30dbd22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18a2c968fe6e4f1ebe73bda7a8254a98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define prompts for image generation\n",
    "prompts = [\n",
    "    \"Watercolor style image of sks person in a spacesuit, face clearly visible\",\n",
    "    \"Watercolor style image of sks person riding horse\",\n",
    "    \"Watercolor potrait of sks person playing cricket\"\n",
    "]\n",
    "\n",
    "# Generate and save images\n",
    "for i, prompt in enumerate(prompts):\n",
    "    image = pipe(prompt, num_inference_steps=30, guidance_scale=7.5).images[0]\n",
    "    image.save(f\"generated_scene_{i+1}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 23813,
     "status": "ok",
     "timestamp": 1746267718144,
     "user": {
      "displayName": "Lahari Kethu",
      "userId": "01960283194028296161"
     },
     "user_tz": -330
    },
    "id": "weY-gyXpNMf7",
    "outputId": "511c782a-326c-4402-88b5-ac1e1731a1bc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "# move training_data and output folders into mydrive/realityAI\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "!mkdir -p /content/drive/MyDrive/realityAI\n",
    "!mv /content/training_data /content/drive/MyDrive/realityAI\n",
    "!mv /content/output /content/drive/MyDrive/realityAI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 69,
     "status": "ok",
     "timestamp": 1746265126415,
     "user": {
      "displayName": "Lahari Kethu",
      "userId": "01960283194028296161"
     },
     "user_tz": -330
    },
    "id": "AmMqfhbrR7zY",
    "outputId": "518590f0-340f-4460-b930-ed158c964079"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content\n"
     ]
    }
   ],
   "source": [
    "%cd /content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 113,
     "referenced_widgets": [
      "2b7781a96c514f72bb7dd14df95116b2",
      "375ea72ca79a46a0b6b23b37d16fe946",
      "21cc6b7a687a46c8a939e0f554656e0a",
      "e3d088e488914c4089791e133900b01f",
      "919accaafbf24847b131ccc5336c766b",
      "f95fe2bf029a40968a3d598062cf6335",
      "dcebc4af294d409ca7206a0114d49342",
      "be3854c0ba354e318757cb39d7222667",
      "0697863956e949ffa7bcb6d512d6c7d7",
      "86264efc78684780be793bf31d584eb3",
      "ab3c168f34f84181a2fb9a9909c27c65",
      "ee7ee1d0b81c44f9a514b41d8e25698a",
      "314b5d5fdc7a40ac92450517bce76cb2",
      "b9e557f5b1c24d68aef9c971b5a57ba6",
      "0f835188350d424192adcdbf585ab9c0",
      "aac8a66b4a494cb3be82384a5bc08e58",
      "087c221d96cb4d9ebb5955dbdc71d7fb",
      "347e3d7ee0364b0faada91d9e3977f46",
      "dc5063d348bb43e5a98d12bf26bbd3d2",
      "2e3ff2e74dd2426c93b00d69bd6fc543",
      "1f235e61afbd46ecb51bdb88192b400a",
      "88b8f749ac06445989211eacea8ffd87",
      "bd88c86dd08040fb99d0e5c081470538",
      "23f6459c27904469896fd13c7852e1ec",
      "6013b4319ed74b14a6b55989d76770d0",
      "4ff34aac2ad94768b14b837c8a4d1a03",
      "e67b4858d0194048a45c56e8d00872eb",
      "391eb59c788d492e8dbe961efcf5a127",
      "07b48b6dcd4e4fcd915418d9b13dfe13",
      "23de9d2432584ed181394d6069bdaad9",
      "5a69c4c309fd490593e37c840d3935d4",
      "44a5bb391e004d34a318819598b85232",
      "3191d3f3e77344b7bff26242dc50a36e"
     ]
    },
    "executionInfo": {
     "elapsed": 48547,
     "status": "ok",
     "timestamp": 1746267425574,
     "user": {
      "displayName": "Lahari Kethu",
      "userId": "01960283194028296161"
     },
     "user_tz": -330
    },
    "id": "j0R0Mo77R20D",
    "outputId": "36587d5c-3629-4925-e38f-7329d9b8aa88"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b7781a96c514f72bb7dd14df95116b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee7ee1d0b81c44f9a514b41d8e25698a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd88c86dd08040fb99d0e5c081470538",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompts = [\n",
    "    \"Upper-body watercolor portrait of sks person in a spacesuit, highly detailed sks person face in focus, soft lighting\",\n",
    "    \"watercolor-style painting of sks person, riding a horse.Accurate sks person face description, upper body visible, horse partially shown behind\",\n",
    "    \"Close-up watercolor portrait of sks person playing cricket, focus on face with helmet or cap\"\n",
    "]\n",
    "# Generate and save images\n",
    "for i, prompt in enumerate(prompts):\n",
    "    image = pipe(prompt, num_inference_steps=30, guidance_scale=7.5).images[0]\n",
    "    image.save(f\"generated_scene_{i+1}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "executionInfo": {
     "elapsed": 93,
     "status": "ok",
     "timestamp": 1746267473845,
     "user": {
      "displayName": "Lahari Kethu",
      "userId": "01960283194028296161"
     },
     "user_tz": -330
    },
    "id": "CjQDWt-Yi0s8"
   },
   "outputs": [],
   "source": [
    "# !mv /content/generated_scene_3.png /content/output/\n",
    "# !mv /content/generated_scene_1.png /content/output/\n",
    "!mv /content/generated_scene_2.png /content/output/"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [
    {
     "file_id": "1vWIwC1xFZqNmoCunPrwzST4ZDjCAP_LR",
     "timestamp": 1746263484743
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
